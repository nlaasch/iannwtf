{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Optimisation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "PREFETCH_SIZE = 20\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Binary Function\n",
    "Returns True if the target quality is higher than the entire dataset quality mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default threshold everything above 5 is good \n",
    "def make_binary(target, threshold = 5):\n",
    "  return int(target > threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What keys are there and what should be input and target for our NN\n",
    "#### Inputs\n",
    "fixed acidity    \n",
    "volatile acidity       \n",
    "citric acid       \n",
    "residual sugar       \n",
    "chlorides       \n",
    "free sulfur dioxide       \n",
    "total sulfur dioxide       \n",
    "density       \n",
    "pH       \n",
    "sulphates       \n",
    "alcohol       \n",
    "\n",
    "#### Target\n",
    "quality     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "csv_file = tf.keras.utils.get_file('winequality-red.csv', 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity           float64\n",
      "volatile acidity        float64\n",
      "citric acid             float64\n",
      "residual sugar          float64\n",
      "chlorides               float64\n",
      "free sulfur dioxide     float64\n",
      "total sulfur dioxide    float64\n",
      "density                 float64\n",
      "pH                      float64\n",
      "sulphates               float64\n",
      "alcohol                 float64\n",
      "quality                   int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV to Tensorflow with pandas\n",
    "df = pd.read_csv(csv_file, sep = ';')\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "df_target = df['quality']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset and pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "full_size = len(df)\n",
    "\n",
    "train_size = int(0.7 * full_size)\n",
    "valid_size = int(0.15 * full_size)\n",
    "\n",
    "train_ds, valid_ds, test_ds = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [train_size,train_size + valid_size])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate lables from input\n",
    "train_label = train_ds[\"quality\"]\n",
    "train_input = train_ds.drop(\"quality\", axis = 1)\n",
    "\n",
    "test_label = test_ds[\"quality\"]\n",
    "test_input = test_ds.drop(\"quality\", axis = 1)\n",
    "\n",
    "validate_label = valid_ds[\"quality\"]\n",
    "validate_input = valid_ds.drop(\"quality\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tensorflow dataset\n",
    "train_ds =tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((validate_input, validate_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline\n",
    "def prepare_data(dataset):\n",
    "  # Create binary target values\n",
    "  dataset = dataset.map(lambda inputs , target: (inputs, make_binary(target)))\n",
    "  # Create batches for input and prefetch for better performance\n",
    "  dataset = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_SIZE)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data pipeline\n",
    "train_ds = prepare_data(train_ds)\n",
    "test_ds = prepare_data(test_ds)\n",
    "valid_ds = prepare_data(valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom model with custom layer\n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    # Initialize model layers\n",
    "    def __init__(self):\n",
    "        #Inheret __init__ tf.keras.model\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Input layer with sigmoid activation function and a L_1 + L_2 regularization (regularization factors recommended by Keras)\n",
    "        self.layer1 = Dense(64, activation=tf.nn.sigmoid, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5))\n",
    "        # Add a dropout layer expecting a tensor of any rank, turning random inputs from layer1 to zero (Dropout rate = 0.5)\n",
    "        self.layer2 = Dropout(0.5)\n",
    "        # binary activation function for output\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    \n",
    "    # cast the call-function as tf.function to increase efficiency\n",
    "    @tf.function\n",
    "    # pass the input through the layers of the network and return the output\n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        # Activate training mode -> set some input to zero\n",
    "        x = self.layer2(x, True)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old graph and start new training\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss of an input for the model and optimize/tweak according the parameters\n",
    "def train_step(model, input, target, loss_function, optimizer, print_w = False):\n",
    "\n",
    "  # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "  with tf.GradientTape() as tape:\n",
    "      prediction = model(input)\n",
    "      loss = loss_function(target, prediction)\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  # Gradients get applied to individual weights    \n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  # Print all weights of respective layer (here first one)\n",
    "  if print_w == True:\n",
    "    print(model.layers[0].weights)\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "# compute the differences between or model prediction and the label, -> Supervision\n",
    "def test(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "  test_accuracy_aggregator = []\n",
    "  test_loss_aggregator = []\n",
    "\n",
    "  for (input, target) in test_data:\n",
    "    prediction = model(input)\n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    sample_test_accuracy =  np.round(target, 0) == np.round(prediction, 0)\n",
    "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "    \n",
    "  # for all input and computed losses get the mean of accuracy and loss and return them\n",
    "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 starting with accuracy tf.Tensor(0.4465384615384615, shape=(), dtype=float64)\n",
      "Epoch: 1 starting with accuracy tf.Tensor(0.545, shape=(), dtype=float64)\n",
      "Epoch: 2 starting with accuracy tf.Tensor(0.5738461538461539, shape=(), dtype=float64)\n",
      "Epoch: 3 starting with accuracy tf.Tensor(0.5734615384615385, shape=(), dtype=float64)\n",
      "Epoch: 4 starting with accuracy tf.Tensor(0.5676923076923077, shape=(), dtype=float64)\n",
      "Epoch: 5 starting with accuracy tf.Tensor(0.5715384615384616, shape=(), dtype=float64)\n",
      "Epoch: 6 starting with accuracy tf.Tensor(0.5546153846153846, shape=(), dtype=float64)\n",
      "Epoch: 7 starting with accuracy tf.Tensor(0.5684615384615385, shape=(), dtype=float64)\n",
      "Epoch: 8 starting with accuracy tf.Tensor(0.5703846153846154, shape=(), dtype=float64)\n",
      "Epoch: 9 starting with accuracy tf.Tensor(0.5700000000000001, shape=(), dtype=float64)\n",
      "Epoch: 10 starting with accuracy tf.Tensor(0.5611538461538461, shape=(), dtype=float64)\n",
      "Epoch: 11 starting with accuracy tf.Tensor(0.5676923076923077, shape=(), dtype=float64)\n",
      "Epoch: 12 starting with accuracy tf.Tensor(0.5661538461538462, shape=(), dtype=float64)\n",
      "Epoch: 13 starting with accuracy tf.Tensor(0.5611538461538461, shape=(), dtype=float64)\n",
      "Epoch: 14 starting with accuracy tf.Tensor(0.5746153846153846, shape=(), dtype=float64)\n",
      "tf.Tensor(0.5665384615384615, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Predefine learning-rate and epochs\n",
    "num_epochs = 15\n",
    "alpha = 0.01\n",
    "\n",
    "# Create a model\n",
    "model = CustomModel()\n",
    "\n",
    "# Define loss-function and optimizer\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Collection of relevant optimizers from week 4\n",
    "optimizer = tf.keras.optimizers.Adam(alpha)\n",
    "#optimizer = tf.keras.optimizers.SGD(alpha)\n",
    "#optimizer = tf.keras.optimizers.RMSprop(alpha)\n",
    "\n",
    "# create empty arrays to store test/accuracy values, to track the network progress\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# get initial accuracy- and loss valus before training starts\n",
    "test_loss, test_accuracy = test(model, test_ds, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "train_loss, _ = test(model, train_ds, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # print accuracy of each epoch\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {str(test_accuracies[-1])}')\n",
    "    \n",
    "    loss_epoch = []\n",
    "    # for all input, do a forwardstep and obtain loss\n",
    "    for input, target in train_ds:\n",
    "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
    "        loss_epoch.append(train_loss)\n",
    "    # get the mean loss of this epoch by using reduce_sum of TF over all input-losses and appending to the array  \n",
    "    train_losses.append(tf.reduce_mean(loss_epoch))\n",
    "    \n",
    "    # get the losses and accuracy of this epoch and store them\n",
    "    test_loss, test_accuracy = test(model, test_ds, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "# print accuracy after 10 epochs\n",
    "print(test_accuracies[-1])\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ce5b025ee46dfd59c93f3793a6f842c4d262f1d9a3727dc60bd3e533930326"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf_m1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
