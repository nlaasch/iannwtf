{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import time\n",
    "import re\n",
    "from tensorflow.keras.layers import Layer, Embedding, Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bible.txt', 'r') as file:\n",
    "    bible = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(text,k = 10000, splitter = \"whitespace\", with_dot = False):\n",
    "    p = text.lower()\n",
    "    \n",
    "    if(with_dot):\n",
    "        p = re.sub('[^A-Za-z. ]+', '', p)\n",
    "    else: \n",
    "        p = re.sub('[^A-Za-z ]+', '', p)\n",
    "    \n",
    "    split_it = p.split()\n",
    "    counter = Counter(split_it)\n",
    "    most_occur = counter.most_common(k)\n",
    "\n",
    "    p = ' '.join(p.split())\n",
    "\n",
    "    if splitter == \"whitespace\":\n",
    "        splitter = tf_text.WhitespaceTokenizer()\n",
    "        p = splitter.split(p)  \n",
    "\n",
    "\n",
    "    return p, split_it, most_occur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbible, ogbible, most_occur = prep(bible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'the' b'first' b'book' ... b'you' b'all' b'amen'], shape=(790017,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(pbible)\n",
    "#print(type(ogbible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(word, wlist):\n",
    "    matched_indeces = []\n",
    "    i = 0\n",
    "    length = len(wlist)\n",
    "    while i < length:\n",
    "        if word == wlist[i]:\n",
    "            matched_indeces.append(i)\n",
    "        i += 1\n",
    "    return matched_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(indexer('the', ogbible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(word, bible, k = 4):\n",
    "    scope = []\n",
    "    indexes = indexer(word, bible)\n",
    "    index = indexes[random.randint(0, len(indexes)-1)]\n",
    "    for i in range(int(k/2)):\n",
    "        scope.append((word,bible[index-(i+1)]))\n",
    "        scope.append((word,bible[index+(i+1)]))\n",
    "    return scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_context() needs to work with most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('jesus', 'and')\n"
     ]
    }
   ],
   "source": [
    "#print(find_context('jesus', ogbible)[0])\n",
    "print(find_context('jesus', most_occur)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'first', 'book', 'of', 'moses']\n"
     ]
    }
   ],
   "source": [
    "#print(most_occur[i][0])\n",
    "print(ogbible[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1160/2869776466.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mogbible\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1160/2869776466.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(bible, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbible\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbible\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1160/4111501421.py\u001b[0m in \u001b[0;36mfind_context\u001b[1;34m(word, bible, k)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbible\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbible\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def create_dataset(bible, k = 10000):\n",
    "    \n",
    "    pairs = []\n",
    "    for word in bible:\n",
    "        pairs.append(find_context(word, bible))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "print(create_dataset(ogbible[:100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F체r dummies:\n",
    "(target word, context word) in tensor and selbem index.\n",
    "\n",
    "Wir feeden einen onehot vektor f체r das target word und createn einen embedded vector (ver채ndert von weights layer one).  Embedded vector ist deutlich kleiner als der input.\n",
    "Der kommt in einen output layer. In dem wird softmax activation auf den embedded vector angewandt.\n",
    "\n",
    "F체r loss vergleichen wir output mit onehot vector von target mit cross-entropy.\n",
    "\n",
    "\n",
    "\n",
    "negative sampling:\n",
    "Turns skip-gram into binary classification task by having real (pos) and fake(negative) examples. \n",
    "The objective of the network now is to distinct them.\n",
    "\n",
    "negative examples have the the same target-word as the example before but a random selected context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess to return pairs onehot encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedder = Embedding(vocab_size, 64)\n",
    "        self.classifier = Embedding(64, vocab_size)\n",
    "        self.activation = Softmax()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, input):\n",
    "        x = self.embedder(input)\n",
    "        x = self.classifier(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss of an input for the model and optimize/tweak according the parameters\n",
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# compute the differences between or model prediction and the label, -> Supervision\n",
    "def test(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "  test_loss_aggregator = []\n",
    "  for (input, target) in test_data:\n",
    "\n",
    "    prediction = model(input)\n",
    "    \n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    \n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    \n",
    "# for all input and computed losses get the mean of accuracy and loss and return them\n",
    "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "\n",
    "  return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe categorical cross entropy\n",
    "training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predefine learning-rate and epochs\n",
    "num_epochs = 10\n",
    "alpha = 0.001\n",
    "\n",
    "# create a model\n",
    "model = Embedder()\n",
    "model.summary\n",
    "# define loss-function and optimizer\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# create empty arrays to store test/accuracy values, to track the network progress\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# get initial accuracy- and loss valus before training starts\n",
    "test_loss= test(model, test_ds, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "\n",
    "train_loss= test(model, train_ds, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "print(\"Starting Training AutoEncoder \")\n",
    "# training loop\n",
    "average_time = []\n",
    "for epoch in range(num_epochs):\n",
    "    # print accuracy of each epoch\n",
    "    pre_train_time = time.time()\n",
    "    \n",
    "    loss_epoch = []\n",
    "    # for all input, do a forwardstep and obtain loss\n",
    "    for input, target in tqdm(train_ds_noisy):\n",
    "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
    "        loss_epoch.append(train_loss)\n",
    "    # get the mean loss of this epoch by using reduce_sum of TF over all input-losses and appending to the array  \n",
    "    train_losses.append(tf.reduce_mean(loss_epoch))\n",
    "    \n",
    "    # get the losses and accuracy of this epoch and store them\n",
    "    test_loss = test(model, test_ds_noisy, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    average_time.append(time.time() - pre_train_time)\n",
    "    print(\"Took: \" + str(time.time() - pre_train_time))\n",
    "    print(\"Loss for this epoch: \" + str(test_loss))\n",
    "    \n",
    "# print accuracy after 10 epochs\n",
    "print(\"Mean Time per Epoch: \" + str(round(np.mean(average_time), 2)) + \"! With Parameters: \")\n",
    "\n",
    "#print(\"Batch size: \" + str(BATCH_SIZE) + \"  Prefetch size: \" + str(PREFETCH_SIZE) + \"  Buffer size: \" + str(BUFFER_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " nearest neighbours according to the(cosine similarity) of each epoch\n",
    "\n",
    " 1. Calculate the cosine similarities between the whole embedding and the\n",
    "embedding of the words you want to investigate\n",
    "2. For each selected word, sort the neighbours by their distance and return\n",
    "the k-nearest ones."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e37ec667b037d87ec56a87d451ea53809bb95d77f8477e606005eed6cafb565"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf_m1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
