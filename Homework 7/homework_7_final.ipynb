{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ici7CWS3kOjH"
      },
      "source": [
        "# Homework 7 Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzzyN2jzkOjK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import initializers, Model\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTAa4NlpkOjM"
      },
      "outputs": [],
      "source": [
        "# Defining Hyperparameters\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "PREFETCH_SIZE = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o_QLVgAkOjM"
      },
      "source": [
        "# Creating a Generation Function and Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZmgNx8HkOjM"
      },
      "outputs": [],
      "source": [
        "# Generate random noise \n",
        "def my_integration_task(seq_len, num_samples):\n",
        "  num=0\n",
        "  while num < num_samples:\n",
        "    x=np.random.normal(0,1,seq_len)\n",
        "    y=np.expand_dims(x,-1)\n",
        "    num+=1\n",
        "    sum=np.sum(x,axis=0)\n",
        "    if (sum>1.0):\n",
        "      target=1\n",
        "      yield y,target\n",
        "    else:\n",
        "      target=0\n",
        "      yield y,target\n",
        "\n",
        "\n",
        "# create wrapper to feed to data_from_generator function of tensorflow\n",
        "def integration_wrapper():\n",
        "  for e in my_integration_task(25,80.000):\n",
        "    yield e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVCsGJowkOjN"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PFW4AqtkOjO",
        "outputId": "d8dda7cb-4319-475b-b625-72ffc645fab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-12 22:27:25.568541: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2021-12-12 22:27:25.568651: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# create 2 datasets from generator\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(integration_wrapper, output_types=(tf.float32, tf.float32))\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(integration_wrapper, output_types=(tf.float32, tf.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOl_h4VjkOjO"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMV8VcBEkOjP"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    # Shuffle, batch and prefetch\n",
        "    data = data.cache().shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE).prefetch(PREFETCH_SIZE)\n",
        "    return data\n",
        "    \n",
        "\n",
        "train_ds = preprocess(train_ds)\n",
        "test_ds = preprocess(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd0o-W2EkOjP"
      },
      "source": [
        "# LSTM Cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQxUbKoJkOjQ"
      },
      "outputs": [],
      "source": [
        "class LSTM_Cell(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LSTM_Cell, self).__init__()\n",
        "    # define amount of units \n",
        "    self.units = units\n",
        "    # forget_gate to drop values out of cell_state\n",
        "    self.f_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, bias_initializer=tf.keras.initializers.Ones)\n",
        "    # input gate and candidates combined into updating cell-state\n",
        "    self.input_gate = tf.keras.layers.Dense(units, activation=tf.nn.tanh, kernel_initializer='orthogonal')\n",
        "    self.candidates_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "    # output filtering gate\n",
        "    self.out_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "  \n",
        "\n",
        "  \n",
        "  def call(self, x, states):\n",
        "    # split states \n",
        "    hidden_state, cell_state = states\n",
        "    # concat hidden_state from t-1 with input at t\n",
        "    concat_input = tf.concat((x, hidden_state), axis=-1)\n",
        "    # apply forget_gate\n",
        "    cell_state = cell_state*self.f_gate(concat_input)\n",
        "    # compute update for cellstate\n",
        "    update = self.input_gate(concat_input)*self.candidates_gate(concat_input)\n",
        "    # update cell_state\n",
        "    cell_state = cell_state + update\n",
        "    # compute new hidden_state / output\n",
        "    out = tf.nn.tanh(cell_state)*self.out_gate(concat_input)\n",
        "    return out, (out, cell_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8fI_Ih2kOjQ"
      },
      "source": [
        "# LSTM Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI4n_Hb0kOjR"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTM(Layer):\n",
        "  # initialize the class with cell\n",
        "  def __init__(self, cell):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.cell = cell\n",
        "  \n",
        "\n",
        "  # call the layer with mutliple time step (seq_len)  \n",
        "  def call(self, x, states):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    outs = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=True)\n",
        "  \t# for every timestep t\n",
        "    for t in tf.range(seq_len):\n",
        "      # compute otuput and hidden_state\n",
        "      t_out, states = self.cell(x[:,t,:], states)\n",
        "      outs = outs.write(t, t_out)\n",
        "    \n",
        "    # transpose and stack the output to return it in [batch_size, seq_len, output_size]\n",
        "    out = outs.stack()\n",
        "    out = tf.transpose(out, perm=[1,0,2])\n",
        "    \n",
        "    return out\n",
        "\n",
        "  # function to reset/return to a zero_state, \n",
        "  def zero_state(self, batch_size):\n",
        "    return (tf.zeros((batch_size, self.cell.units)), tf.zeros((batch_size, self.cell.units)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCFd53qxkOjR"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh9hC01nkOjS"
      },
      "outputs": [],
      "source": [
        "class RNN_Model(Model):\n",
        "    def __init__(self):\n",
        "        super(RNN_Model,self).__init__()\n",
        "        # create 2 filtering input layers\n",
        "        self.input_layer = Dense(64, activation=\"sigmoid\")\n",
        "        self.input_layer2 = Dense(32)\n",
        "        # define a cell with amount of units and asssign the wrapper to it\n",
        "        self.cell= LSTM_Cell(2)\n",
        "        self.lstm_wrapper = LSTM(self.cell)\n",
        "\n",
        "        # activation layer for binary output\n",
        "        self.out = Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, input):\n",
        "        # get input shape and create initial zero_state for LSTM\n",
        "        batch_size = tf.shape(input)[0]\n",
        "        zero_state = self.lstm_wrapper.zero_state(batch_size=batch_size)\n",
        "        # feed input through network layers\n",
        "        x = self.input_layer(input) \n",
        "        x = self.input_layer2(x)\n",
        "        x = self.lstm_wrapper(x, zero_state)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjhtXMVekOjS"
      },
      "source": [
        "# Accucary Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5UNMin-kOjS"
      },
      "outputs": [],
      "source": [
        "# modified accuracy function to fit dimensions\n",
        "@tf.function \n",
        "def train_step(model, input, target, loss_function, optimizer, training=True):\n",
        "  target = tf.expand_dims(target, axis=-1)\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction[:,-1,:])\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  acc = tf.reduce_mean(tf.cast(tf.round(prediction[:,-1,:])==target, dtype=tf.float32))\n",
        "  return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY39FlBxkOjT"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzCzkhqHkOjT",
        "outputId": "649bdc38-4484-4701-c2f4-b87ee8c0dd2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-12 22:27:27.405052: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-12-12 22:27:27.405217: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2021-12-12 22:27:28.595841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "2021-12-12 22:27:32.584476: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss is 0.68233. Accuracy 0.58594\n",
            "Loss is 0.67536. Accuracy 0.60938\n",
            "Loss is 0.69624. Accuracy 0.51562\n",
            "Loss is 0.67357. Accuracy 0.60938\n",
            "Loss is 0.69077. Accuracy 0.53906\n",
            "Loss is 0.70291. Accuracy 0.49219\n",
            "Loss is 0.70251. Accuracy 0.49219\n",
            "Loss is 0.69511. Accuracy 0.51562\n",
            "Loss is 0.68043. Accuracy 0.58594\n",
            "Loss is 0.69317. Accuracy 0.51562\n",
            "Loss is 0.68859. Accuracy 0.53906\n",
            "Loss is 0.69786. Accuracy 0.46875\n",
            "Loss is 0.68785. Accuracy 0.53906\n",
            "Loss is 0.68732. Accuracy 0.53906\n",
            "Loss is 0.68899. Accuracy 0.51562\n",
            "Loss is 0.68805. Accuracy 0.58594\n",
            "Loss is 0.68831. Accuracy 0.51562\n",
            "Loss is 0.69040. Accuracy 0.51562\n",
            "Loss is 0.68503. Accuracy 0.56250\n",
            "Loss is 0.69401. Accuracy 0.46875\n",
            "Loss is 0.68982. Accuracy 0.51562\n",
            "Loss is 0.69030. Accuracy 0.51562\n",
            "Loss is 0.69121. Accuracy 0.46875\n",
            "Loss is 0.68802. Accuracy 0.54688\n",
            "Loss is 0.68814. Accuracy 0.51562\n",
            "Loss is 0.69046. Accuracy 0.55469\n",
            "Loss is 0.69039. Accuracy 0.55469\n",
            "Loss is 0.68501. Accuracy 0.67188\n",
            "Loss is 0.68374. Accuracy 0.51562\n",
            "Loss is 0.68638. Accuracy 0.51562\n",
            "Loss is 0.69126. Accuracy 0.49219\n",
            "Loss is 0.67716. Accuracy 0.56250\n",
            "Loss is 0.69572. Accuracy 0.46875\n",
            "Loss is 0.68669. Accuracy 0.53906\n",
            "Loss is 0.68428. Accuracy 0.54688\n",
            "Loss is 0.68343. Accuracy 0.57031\n",
            "Loss is 0.68610. Accuracy 0.48438\n",
            "Loss is 0.68068. Accuracy 0.60938\n",
            "Loss is 0.68053. Accuracy 0.57031\n",
            "Loss is 0.68396. Accuracy 0.53125\n",
            "Loss is 0.67545. Accuracy 0.58594\n",
            "Loss is 0.67177. Accuracy 0.56250\n",
            "Loss is 0.67583. Accuracy 0.50000\n",
            "Loss is 0.66541. Accuracy 0.77344\n",
            "Loss is 0.67318. Accuracy 0.50781\n",
            "Loss is 0.66322. Accuracy 0.64844\n",
            "Loss is 0.64869. Accuracy 0.56250\n",
            "Loss is 0.63626. Accuracy 0.64844\n",
            "Loss is 0.65158. Accuracy 0.59375\n",
            "Loss is 0.60925. Accuracy 0.74219\n",
            "Loss is 0.61314. Accuracy 0.62500\n",
            "Loss is 0.58646. Accuracy 0.81250\n",
            "Loss is 0.59649. Accuracy 0.67969\n",
            "Loss is 0.61586. Accuracy 0.60156\n",
            "Loss is 0.55919. Accuracy 0.75000\n",
            "Loss is 0.50325. Accuracy 0.89062\n",
            "Loss is 0.53977. Accuracy 0.70312\n",
            "Loss is 0.55720. Accuracy 0.72656\n",
            "Loss is 0.47726. Accuracy 0.84375\n",
            "Loss is 0.52134. Accuracy 0.79688\n",
            "Loss is 0.44875. Accuracy 0.87500\n",
            "Loss is 0.46973. Accuracy 0.85938\n",
            "Loss is 0.42449. Accuracy 0.91406\n",
            "Loss is 0.39356. Accuracy 0.92969\n",
            "Loss is 0.38178. Accuracy 0.94531\n",
            "Loss is 0.44110. Accuracy 0.87500\n",
            "Loss is 0.44467. Accuracy 0.84375\n",
            "Loss is 0.45031. Accuracy 0.84375\n",
            "Loss is 0.39721. Accuracy 0.92188\n",
            "Loss is 0.39044. Accuracy 0.89062\n",
            "Loss is 0.38856. Accuracy 0.89062\n",
            "Loss is 0.35353. Accuracy 0.96094\n",
            "Loss is 0.36141. Accuracy 0.96094\n",
            "Loss is 0.40718. Accuracy 0.89062\n",
            "Loss is 0.41564. Accuracy 0.82812\n",
            "Loss is 0.43536. Accuracy 0.91406\n",
            "Loss is 0.40293. Accuracy 0.89062\n",
            "Loss is 0.36820. Accuracy 0.94531\n",
            "Loss is 0.37641. Accuracy 0.92969\n",
            "Loss is 0.35989. Accuracy 0.92188\n",
            "Loss is 0.38390. Accuracy 0.93750\n",
            "Loss is 0.34744. Accuracy 0.95312\n",
            "Loss is 0.36261. Accuracy 0.89062\n",
            "Loss is 0.41145. Accuracy 0.89062\n",
            "Loss is 0.35168. Accuracy 0.94531\n",
            "Loss is 0.35226. Accuracy 0.93750\n",
            "Loss is 0.37031. Accuracy 0.92188\n",
            "Loss is 0.38141. Accuracy 0.92188\n",
            "Loss is 0.34376. Accuracy 0.95312\n",
            "Loss is 0.31413. Accuracy 0.96875\n",
            "Loss is 0.34517. Accuracy 0.94531\n",
            "Loss is 0.31339. Accuracy 0.96875\n",
            "Loss is 0.32114. Accuracy 0.94531\n",
            "Loss is 0.32249. Accuracy 0.97656\n",
            "Loss is 0.31955. Accuracy 0.96094\n",
            "Loss is 0.33111. Accuracy 0.93750\n",
            "Loss is 0.34529. Accuracy 0.92969\n",
            "Loss is 0.34551. Accuracy 0.95312\n",
            "Loss is 0.32592. Accuracy 0.92969\n",
            "Loss is 0.34865. Accuracy 0.93750\n"
          ]
        }
      ],
      "source": [
        "# define amount of epochs\n",
        "iters = 100\n",
        "#create model instance and define parameters\n",
        "model = RNN_Model()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_function = tf.losses.BinaryCrossentropy()\n",
        "\n",
        "\n",
        "# training loop\n",
        "for iter in range(iters):\n",
        "  loss_agg = []\n",
        "  acc_agg = []\n",
        "  for input, target in train_ds:\n",
        "    loss, acc = train_step(model, input, target, loss_function, optimizer)\n",
        "    loss_agg.append(loss)\n",
        "    acc_agg.append(acc)\n",
        "  print(\"Loss is %2.5f. Accuracy %2.5f\" % (np.mean(loss_agg), np.mean(acc_agg)))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('ann': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "homework_7_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}