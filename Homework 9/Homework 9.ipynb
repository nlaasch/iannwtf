{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds \n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization, Reshape, Conv2DTranspose\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'aircraft carrier', b'airplane', b'alarm clock', b'ambulance', b'angel', b'animal migration', b'ant', b'anvil', b'apple', b'arm']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
    "print(categories[:10])\n",
    "category = 'candle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141545 images to train on\n",
      "2000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Creates a folder to download the original drawings into.\n",
    "# We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
    "\n",
    "if not os.path.isdir('npy_files'):\n",
    "    os.mkdir('npy_files')\n",
    "    \n",
    "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
    "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
    "\n",
    "images = np.load(f'npy_files/{category}.npy')\n",
    "print(f'{len(images)} images to train on')\n",
    "\n",
    "# You can limit the amount of images you use for training by setting :\n",
    "train_images = images[:10000]\n",
    "# You should also define a samller subset of the images for testing..\n",
    "# TODO\n",
    "test_images = images[10000:12000]\n",
    "print(len(test_images))\n",
    "print(len(train_images))\n",
    "# Notice that this to numpy format contains 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shuffle and batching sizes\n",
    "batch_size = 16\n",
    "shuffle_size = 1000\n",
    "prefetch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # # change dtype to float32\n",
    "    # data = data.map(lambda img, label: (tf.cast(img, tf.float32), label))\n",
    "    # # change value range from 0-255 to -1 - 1\n",
    "    # data = data.map(lambda img, label: ((img/128)-1, label))\n",
    "    # # change format to 28,28,1\n",
    "    # data = data.map(lambda img, label: (tf.reshape(img, (28,28,1)), label))\n",
    "\n",
    "    data = data.map(lambda img: tf.cast(img, tf.float32))\n",
    "    # change value range from 0-255 to -1 - 1\n",
    "    data = data.map(lambda img: (img/128)-1)\n",
    "    # change format to 28,28,1\n",
    "    data = data.map(lambda img: tf.reshape(img, (28,28,1)))\n",
    "\n",
    "    # do other prepocessing stuff\n",
    "    data = data.shuffle(shuffle_size).batch(batch_size).prefetch(prefetch_size)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets from tensor\n",
    "labels = np.ones_like(train_images)\n",
    "labels_test = np.ones_like(test_images)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images,labels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, labels_test))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_images)\n",
    "\n",
    "train_ds = preprocess(train_ds)\n",
    "test_ds = preprocess(test_ds)\n",
    "\n",
    "#check data format real quick\n",
    "# for x in test_ds:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Conv2D(32,kernel_size=2, padding='same'),\n",
    "            Conv2D(32, kernel_size=3, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(32, kernel_size=2, padding='same'),\n",
    "            Conv2D(32, kernel_size=2, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "        # self.conv1 = Conv2D(32,kernel_size=2, padding='same')\n",
    "        # self.conv2 = Conv2D(32, kernel_size=3, padding='same')\n",
    "        # self.batch1 = BatchNormalization()\n",
    "\n",
    "        # self.conv3 = Conv2D(32, kernel_size=2, padding='same')\n",
    "        # self.conv4 = Conv2D(32, kernel_size=2, padding='same')\n",
    "        # self.batch2 = BatchNormalization()\n",
    "\n",
    "        # self.out = Dense(1, activation='sigmoid')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training):\n",
    "        for l in self.l:\n",
    "           x = l(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l = [\n",
    "            Dense(49, activation='relu'),\n",
    "            Reshape((7,7,1)),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation=\"relu\", padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(1, kernel_size=2, strides=2, activation='tanh', padding='same')\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-0.0001925 ]\n",
      "   [-0.00108132]\n",
      "   [ 0.00017479]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.00033606]\n",
      "   [ 0.00107834]\n",
      "   [ 0.00057652]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00158187]\n",
      "   [-0.00285763]\n",
      "   [-0.00232631]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.00176244]\n",
      "   [ 0.00565496]\n",
      "   [ 0.00302357]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00829536]\n",
      "   [-0.01498458]\n",
      "   [-0.01219899]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00193295]\n",
      "   [ 0.00212681]\n",
      "   [ 0.00067171]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]\n",
      "\n",
      "\n",
      " [[[-0.00102419]\n",
      "   [-0.00575407]\n",
      "   [ 0.00093022]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.00178845]\n",
      "   [ 0.00573828]\n",
      "   [ 0.00306812]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00841766]\n",
      "   [-0.01520534]\n",
      "   [-0.01237878]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.00047607]\n",
      "   [ 0.00152754]\n",
      "   [ 0.00081675]\n",
      "   ...\n",
      "   [ 0.00268778]\n",
      "   [ 0.00143709]\n",
      "   [-0.00048673]]\n",
      "\n",
      "  [[-0.0022409 ]\n",
      "   [-0.00404813]\n",
      "   [-0.00329552]\n",
      "   ...\n",
      "   [-0.00712264]\n",
      "   [-0.00579848]\n",
      "   [-0.00633662]]\n",
      "\n",
      "  [[-0.00052217]\n",
      "   [ 0.0005745 ]\n",
      "   [ 0.00018146]\n",
      "   ...\n",
      "   [ 0.00101083]\n",
      "   [ 0.00031926]\n",
      "   [ 0.00192563]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00181871]\n",
      "   [ 0.000294  ]\n",
      "   [-0.00582314]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00181369]\n",
      "   [ 0.00096972]\n",
      "   [-0.00032847]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00480635]\n",
      "   [-0.00391277]\n",
      "   [-0.00427594]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00545372]\n",
      "   [ 0.00291603]\n",
      "   [-0.00098758]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.01445156]\n",
      "   [-0.01176508]\n",
      "   [-0.01285687]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00205113]\n",
      "   [ 0.00064784]\n",
      "   [ 0.00390725]]]\n",
      "\n",
      "\n",
      " [[[-0.0022289 ]\n",
      "   [-0.01252169]\n",
      "   [ 0.00202448]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.00389206]\n",
      "   [ 0.01248734]\n",
      "   [ 0.00667697]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.01831715]\n",
      "   [-0.03308104]\n",
      "   [-0.02693404]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.00235657]\n",
      "   [ 0.00756112]\n",
      "   [ 0.00404284]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.01109155]\n",
      "   [-0.02003464]\n",
      "   [-0.01631065]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00258453]\n",
      "   [ 0.00284373]\n",
      "   [ 0.00089815]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]], shape=(16, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "G_Test = Generator()\n",
    "x = tf.random.normal([batch_size,7])\n",
    "x = G_Test(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(G,D,G_optimizer, D_optimizer, data, noise):\n",
    "    with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "\n",
    "        fake_data = G(noise)\n",
    "        fake_data_pred = D(fake_data)\n",
    "        real_data_pred = D(data)\n",
    "\n",
    "        D_loss = -tf.math.reduce_mean( tf.math.log(real_data_pred) + tf.math.log(1-fake_data_pred) )    \n",
    "        G_loss = tf.math.reduce_mean( tf.math.log(1-fake_data_pred) )\n",
    "\n",
    "        D_gradients = D_tape.gradient(D_loss, D.trainable_variables)\n",
    "        D_optimizer.apply_gradients(zip(D_gradients, D.trainable_variables))\n",
    "\n",
    "        G_gradients = G_tape.gradient(G_loss, G.trainable_variables)\n",
    "        G_optimizer.apply_gradients(zip(G_gradients, G.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe seperate test function\n",
    "def test(D,G, data, noise):\n",
    "\n",
    "    fake_data = G(noise)\n",
    "    fake_data_pred = D(fake_data)\n",
    "    real_data_pred = D(data)\n",
    "\n",
    "    D_loss = -tf.math.reduce_mean( tf.math.log(real_data_pred) + tf.math.log(1-fake_data_pred) )    \n",
    "    G_loss = tf.math.reduce_mean( tf.math.log(1-fake_data_pred) )\n",
    "\n",
    "    return D_loss, G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firing up Generator and Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]2022-01-14 11:54:04.120178: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:54:04.151896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:54:04.246306: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:54:04.346127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:54:04.498935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|██████████| 625/625 [00:23<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6709158>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3604752>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:22<00:00, 27.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6777355>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3741319>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:22<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6785724>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3727798>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:22<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6756505>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3717824>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:23<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.26974306>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=0.559566>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:22<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.57315314>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.1572971>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:24<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6875745>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3794897>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:25<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.69037306>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3757136>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:23<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6858362>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3664916>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:24<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=-0.6907184>]\n",
      "Discriminator Loss for this epoch is: [<tf.Tensor: shape=(), dtype=float32, numpy=1.3795111>]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "D_optimizer = tf.keras.optimizers.Adam()\n",
    "G_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_noise = tf.random.normal([batch_size,7])\n",
    "\n",
    "\n",
    "print(\"Firing up Generator and Discriminator\")\n",
    "for epoch in range(num_epochs):       \n",
    "    losses_gen = []\n",
    "    losses_dis = []     \n",
    "    loss_epoch_gen = []\n",
    "    loss_epoch_dis = []\n",
    "    for data in tqdm(train_ds):\n",
    "        z = tf.random.normal([batch_size,7])\n",
    "        #train_step(G,D,G_optimizer=G_optimizer, D_optimizer=D_optimizer, data=data, noise=z)\n",
    "\n",
    "    \n",
    "        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "        \n",
    "            \n",
    "            fake_data = G(z)\n",
    "            fake_data_pred = D(fake_data)\n",
    "            real_data_pred = D(data)\n",
    "            \n",
    "            D_loss = -tf.math.reduce_mean( tf.math.log(real_data_pred) + tf.math.log(1-fake_data_pred) )\n",
    "            \n",
    "            G_loss = tf.math.reduce_mean( tf.math.log(1-fake_data_pred) )\n",
    "\n",
    "            losses_gen.append(G_loss)\n",
    "            losses_dis.append(D_loss)\n",
    "            #loss_epoch_gen.append(G_loss)\n",
    "            D_gradients = D_tape.gradient(D_loss, D.trainable_variables)\n",
    "            D_optimizer.apply_gradients(zip(D_gradients, D.trainable_variables))\n",
    "\n",
    "            G_gradients = G_tape.gradient(G_loss, G.trainable_variables)\n",
    "            G_optimizer.apply_gradients(zip(G_gradients, G.trainable_variables))\n",
    "\n",
    "    loss_epoch_gen.append(tf.reduce_mean(losses_gen))\n",
    "    loss_epoch_dis.append(tf.reduce_mean(losses_dis))\n",
    "\n",
    "    print(\"Generator Loss for this epoch is: \" + str(loss_epoch_gen))\n",
    "    print(\"Discriminator Loss for this epoch is: \" + str(loss_epoch_dis))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ann': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
