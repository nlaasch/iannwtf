{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds \n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization, Reshape, Conv2DTranspose\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'aircraft carrier', b'airplane', b'alarm clock', b'ambulance', b'angel', b'animal migration', b'ant', b'anvil', b'apple', b'arm']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
    "print(categories[:10])\n",
    "category = 'candle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141545 images to train on\n",
      "2000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Creates a folder to download the original drawings into.\n",
    "# We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
    "\n",
    "if not os.path.isdir('npy_files'):\n",
    "    os.mkdir('npy_files')\n",
    "    \n",
    "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
    "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
    "\n",
    "images = np.load(f'npy_files/{category}.npy')\n",
    "print(f'{len(images)} images to train on')\n",
    "\n",
    "# You can limit the amount of images you use for training by setting :\n",
    "train_images = images[:10000]\n",
    "# You should also define a samller subset of the images for testing..\n",
    "# TODO\n",
    "test_images = images[10000:12000]\n",
    "print(len(test_images))\n",
    "print(len(train_images))\n",
    "# Notice that this to numpy format contains 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shuffle and batching sizes\n",
    "batch_size = 16\n",
    "shuffle_size = 1000\n",
    "prefetch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # # change dtype to float32\n",
    "    # data = data.map(lambda img, label: (tf.cast(img, tf.float32), label))\n",
    "    # # change value range from 0-255 to -1 - 1\n",
    "    # data = data.map(lambda img, label: ((img/128)-1, label))\n",
    "    # # change format to 28,28,1\n",
    "    # data = data.map(lambda img, label: (tf.reshape(img, (28,28,1)), label))\n",
    "\n",
    "    data = data.map(lambda img: tf.cast(img, tf.float32))\n",
    "    # change value range from 0-255 to -1 - 1\n",
    "    data = data.map(lambda img: (img/128)-1)\n",
    "    # change format to 28,28,1\n",
    "    data = data.map(lambda img: tf.reshape(img, (28,28,1)))\n",
    "\n",
    "    # do other prepocessing stuff\n",
    "    data = data.shuffle(shuffle_size).batch(batch_size).prefetch(prefetch_size)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets from tensor\n",
    "labels = np.ones_like(train_images)\n",
    "labels_test = np.ones_like(test_images)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images,labels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, labels_test))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_images)\n",
    "\n",
    "train_ds = preprocess(train_ds)\n",
    "test_ds = preprocess(test_ds)\n",
    "\n",
    "#check data format real quick\n",
    "# for x in test_ds:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Conv2D(32,kernel_size=2, padding='same'),\n",
    "            Conv2D(32, kernel_size=3, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(32, kernel_size=2, padding='same'),\n",
    "            Conv2D(32, kernel_size=2, padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "        # self.conv1 = Conv2D(32,kernel_size=2, padding='same')\n",
    "        # self.conv2 = Conv2D(32, kernel_size=3, padding='same')\n",
    "        # self.batch1 = BatchNormalization()\n",
    "\n",
    "        # self.conv3 = Conv2D(32, kernel_size=2, padding='same')\n",
    "        # self.conv4 = Conv2D(32, kernel_size=2, padding='same')\n",
    "        # self.batch2 = BatchNormalization()\n",
    "\n",
    "        # self.out = Dense(1, activation='sigmoid')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training):\n",
    "        for l in self.l:\n",
    "           x = l(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l = [\n",
    "            Dense(49, activation='relu'),\n",
    "            Reshape((7,7,1)),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation=\"relu\", padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(32, kernel_size=(2,2), strides=2, activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(1, kernel_size=2, strides=2, activation='tanh', padding='same')\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-0.00223725]\n",
      "   [-0.00234985]\n",
      "   [-0.00420916]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00072401]\n",
      "   [-0.0057434 ]\n",
      "   [-0.00539525]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00305333]\n",
      "   [ 0.00334985]\n",
      "   [-0.00512146]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.00035597]\n",
      "   [-0.00282394]\n",
      "   [-0.00265278]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00150131]\n",
      "   [ 0.00164708]\n",
      "   [-0.00251812]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.0024853 ]\n",
      "   [-0.00528762]\n",
      "   [-0.00164627]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.00067079]\n",
      "   [-0.00532127]\n",
      "   [-0.00499866]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00282889]\n",
      "   [ 0.00310361]\n",
      "   [-0.00474506]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[-0.00468312]\n",
      "   [-0.00996349]\n",
      "   [-0.00310215]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00743334]\n",
      "   [-0.00698273]\n",
      "   [-0.00912805]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00433548]\n",
      "   [-0.00662841]\n",
      "   [-0.00988828]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.01391782]\n",
      "   [-0.00433345]\n",
      "   [-0.01033887]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00236796]\n",
      "   [-0.00222444]\n",
      "   [-0.00290784]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00138104]\n",
      "   [-0.00211156]\n",
      "   [-0.00315004]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00443383]\n",
      "   [-0.00138047]\n",
      "   [-0.00329357]]]\n",
      "\n",
      "\n",
      " [[[-0.00337694]\n",
      "   [-0.00354694]\n",
      "   [-0.00635344]\n",
      "   ...\n",
      "   [-0.00018469]\n",
      "   [-0.00033077]\n",
      "   [ 0.0003105 ]]\n",
      "\n",
      "  [[-0.00109282]\n",
      "   [-0.00866914]\n",
      "   [-0.00814367]\n",
      "   ...\n",
      "   [-0.00045132]\n",
      "   [-0.00042397]\n",
      "   [-0.00055424]]\n",
      "\n",
      "  [[-0.00460879]\n",
      "   [ 0.00505632]\n",
      "   [-0.00773042]\n",
      "   ...\n",
      "   [ 0.0002632 ]\n",
      "   [-0.00040246]\n",
      "   [-0.0006004 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.0015153 ]\n",
      "   [-0.01202055]\n",
      "   [-0.01129194]\n",
      "   ...\n",
      "   [-0.00442292]\n",
      "   [-0.00415486]\n",
      "   [-0.00543137]]\n",
      "\n",
      "  [[-0.00639062]\n",
      "   [ 0.0070112 ]\n",
      "   [-0.01071901]\n",
      "   ...\n",
      "   [ 0.00257963]\n",
      "   [-0.00394401]\n",
      "   [-0.00588375]]\n",
      "\n",
      "  [[-0.01057917]\n",
      "   [-0.02250506]\n",
      "   [-0.00700782]\n",
      "   ...\n",
      "   [-0.00828154]\n",
      "   [-0.00257845]\n",
      "   [-0.00615183]]]\n",
      "\n",
      "\n",
      " [[[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00174271]\n",
      "   [-0.00312161]\n",
      "   [ 0.00293026]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00425941]\n",
      "   [-0.00400123]\n",
      "   [-0.00523058]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00248427]\n",
      "   [-0.00379816]\n",
      "   [-0.00566623]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.00870547]\n",
      "   [-0.00817778]\n",
      "   [-0.01069021]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [ 0.00507756]\n",
      "   [-0.0077628 ]\n",
      "   [-0.01158051]]\n",
      "\n",
      "  [[ 0.        ]\n",
      "   [ 0.        ]\n",
      "   [ 0.        ]\n",
      "   ...\n",
      "   [-0.01629944]\n",
      "   [-0.00507509]\n",
      "   [-0.01210819]]]], shape=(16, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "G_Test = Generator()\n",
    "x = tf.random.normal([batch_size,7])\n",
    "x = G_Test(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 11:02:14.476940: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:02:14.494698: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:02:14.668821: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:02:14.730962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-14 11:02:14.823639: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "D_optimizer = tf.keras.optimizers.Adam()\n",
    "G_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(5):            \n",
    "\n",
    "    for data in train_ds:\n",
    "            \n",
    "        z = tf.random.normal([batch_size,7])\n",
    "\n",
    "        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "            \n",
    "            fake_data = G(z)\n",
    "            fake_data_pred = D(fake_data)\n",
    "            real_data_pred = D(data)\n",
    "            \n",
    "            D_loss = -tf.math.reduce_mean( tf.math.log(real_data_pred) + tf.math.log(1-fake_data_pred) )\n",
    "            \n",
    "            G_loss = tf.math.reduce_mean( tf.math.log(1-fake_data_pred) )\n",
    "            \n",
    "            \n",
    "            D_gradients = D_tape.gradient(D_loss, D.trainable_variables)\n",
    "            D_optimizer.apply_gradients(zip(D_gradients, D.trainable_variables))\n",
    "\n",
    "            G_gradients = G_tape.gradient(G_loss, G.trainable_variables)\n",
    "            G_optimizer.apply_gradients(zip(G_gradients, G.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ann': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
